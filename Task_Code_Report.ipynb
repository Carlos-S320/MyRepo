{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uub82Nr_3Lv_"
   },
   "source": [
    "# Project Report: Income Prediction and Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XnaZMm-o3Q3Z"
   },
   "source": [
    "## Data Preparation and Preprocessing\n",
    "\n",
    "Pandas was used to import bank customer data into Jupyter notebook. It was done before the study. A first inspection of the data showed a column called \"ID,\" which was unhelpful and was immediately eliminated to eliminate unnecessary information. One-hot encoding was also used to represent categorical attributes like education and account ownership. This improvement allowed regression and neural network algorithms to understand non-numerical data.\n",
    "\n",
    "After that, the Interquartile Range (IQR) method was used to remove Mortgage(Thousands) column outliers. This step reduced noise that could disrupt model learning to ensure a cleaner and more robust training dataset. The feature distributions were normalized by scaling the data with StandardScaler. This is crucial to enhancing neural network performance, especially when features vary greatly.\n",
    "\n",
    "The dataset was cleaned and updated before being split into training and testing sets. This allowed proper model evaluation on unreleased data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ekb_FI0e3Xmi"
   },
   "source": [
    "## Regression and Neural Network Modeling\n",
    "\n",
    "Two models were utilized in order to make a prediction regarding the Income(Thousands) variable. These models included a feedforward Neural Network and a conventional Linear Regression model. The linear model was used as a baseline, and it achieved a Mean Squared Error (MSE) value of 874.21. However, despite the fact that this performance was satisfactory, it brought to light shortcomings in the ability to capture complicated relationships within the data.\n",
    "\n",
    "After that, TensorFlow and Keras were utilized in order to build a Neural Network that was advanced in its capabilities. The architecture was trained over a period of ten epochs and consisted of many dense layers that were activated by ReLU. This initial model demonstrated the value of non-linear learning capabilities by displaying a lower MSE of 785.77 when compared to the original model.\n",
    "\n",
    "An additional phase of hyperparameter tweaking was performed with the help of a bespoke wrapper that was compatible with GridSearchCV in order to further increase performance. For this reason, it was possible to systematically optimize both the batch size and the optimizer type. The model that performed the best utilized the Adam optimizer with a batch size of 128, which resulted in a marginally improved MSE of 780.17 after the optimization process. In spite of the fact that deep learning gives advantages, the dataset may be either simple or too large to fully utilize the capabilities of neural networks, as indicated by the marginal gains.\n",
    "\n",
    "Following careful consideration, the neural network model was selected as the ultimate predictive algorithm due to the fact that it possesses higher accuracy and versatility. Because it was able to simulate non-linear interactions, it was better equipped to capture the underlying patterns that were affecting the income of customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ns4USE3a3aux"
   },
   "source": [
    "## Income Prediction for a New Customer\n",
    "\n",
    "To demonstrate the predictive capability of the finalized neural network model, a hypothetical customer profile was constructed. The profile was based on a 45-year-old individual with 20 years of experience, a mortgage of $150,000, a credit score of 700, and other personal and banking features set appropriately.\n",
    "\n",
    "This new data point was processed using the same one-hot encoding structure and standardization scaler applied to the training data. When passed through the optimized neural network model, the predicted income for the customer was 13,029.01 (in thousands), or approximately $13 million, which seems extraordinarily high. This suggests potential issues with the scale or structure of the original target variable and may warrant further data validation or log transformation for better interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X_tCBjQX3dNU"
   },
   "source": [
    "## Sentiment and Semantic Analysis of YouTube Comments\n",
    "\n",
    "The project also included a semantic analysis component to examine viewer sentiment from a YouTube video about India-Pakistan conflict. A total of 100 top comments were extracted using the YoutubeCommentDownloader library. The text data underwent thorough cleaning, including removing URLs, punctuation, digits, and converting text to lowercase for uniformity.\n",
    "\n",
    "Using the TextBlob library, the comments were classified into three sentiment categories: positive, neutral, and negative. Based on the analysis, it was determined that the bulk of the comments were neither positive nor negative, followed by neutral comments. This shows that the viewers were primarily passive or informational in tone, rather than emotionally charged, during the presentation, despite the fact that the subject matter was sensitive.\n",
    "\n",
    "In addition to this, an emotion classification was carried out with the help of a HuggingFace transformer model that had been pre-trained (j-hartmann/emotion-english-distilroberta-base). Despite the fact that certain comments were returned with a 'unknown' classification due to their brevity or the absence of emotional markers, this model nevertheless supplied a rich emotional context that went beyond basic sentiment.\n",
    "\n",
    "A bar chart was used to represent the distribution of sentiment, and it was able to clearly demonstrate that neutral sentiment was the predominant sentiment. A word cloud was also created in order to illustrate the words that were used the most frequently throughout all of the comments. The political and conflict-related aspect of the video content was reflected in the use of notable terms such as \"war,\" \"india,\" \"pakistan,\" \"modi,\" and \"attack.\" These particular words contribute to the reinforcement of the context of the conversation as well as the types of feelings that are likely to be expressed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k9FlFu9G3hPY"
   },
   "source": [
    "## Conclusion and Final Thoughts\n",
    "\n",
    "This project combined structured income prediction and unstructured sentiment analysis to explore both numerical and textual data. In the regression task, while linear regression provided a useful baseline, the neural network outperformed it in terms of MSE and adaptability. The income prediction for a new customer further validated the model's applicability in a real-world scenario, although scale issues may need further investigation.\n",
    "\n",
    "On the semantic analysis front, the integration of classical sentiment scoring and transformer-based emotion recognition allowed for a rich understanding of viewer opinions and emotional tone. Visualization techniques such as bar charts and word clouds helped to summarize the findings effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DcygXIBy3OJu"
   },
   "source": [
    "# PROJECT CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YpQzf6JopVJW"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "id": "o0wycorZpw18",
    "outputId": "fc8cfdac-9546-4d4d-e650-7da8fd3d4af0"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/content/BankRecords.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523
    },
    "id": "-8BrH5ZepzW8",
    "outputId": "cce33537-77f6-43d5-ce98-1e2f51311903"
   },
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gqCCsBlXp1eL",
    "outputId": "c41098bc-faf8-4f47-b76e-82229e6d09b9"
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PAI_etmSp2WO"
   },
   "outputs": [],
   "source": [
    "data = data.drop(['ID'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zWzNLiWLp5ab",
    "outputId": "505008b4-09fc-42d6-f736-ee7523a52381"
   },
   "outputs": [],
   "source": [
    "# Identify categorical columns\n",
    "categorical_cols = data.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "# Print unique values in each categorical column\n",
    "for col in categorical_cols:\n",
    "    print(f\"Unique values in '{col}':\")\n",
    "    print(data[col].unique())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "id": "Y3bG4nefqIu7",
    "outputId": "46af5db3-3426-408b-c7d6-fea88b9ab831"
   },
   "outputs": [],
   "source": [
    "# One-hot encode categorical columns\n",
    "df_encoded = pd.get_dummies(data, columns=categorical_cols, drop_first=False)\n",
    "\n",
    "# Preview the resulting dataframe\n",
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "TJn1JJ1ZqMxN",
    "outputId": "b68dbd15-c5be-431f-a381-95b4638013fa"
   },
   "outputs": [],
   "source": [
    "df_encoded.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NXhfIe_jqjwW"
   },
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "df_encoded = df_encoded.rename(columns={\n",
    "    'Experience(Years)': 'Experience_Years',\n",
    "    'Income(Thousands\\'s)': 'Income_Thousands',\n",
    "    'Mortgage(Thousands\\'s)': 'Mortgage_Thousands'\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "id": "YI6tMgg4qSnM",
    "outputId": "a8252fa7-2f47-47ec-87ac-381e000570c4"
   },
   "outputs": [],
   "source": [
    "# Calculate Q1 and Q3 for the 'mortgage' column\n",
    "Q1 = df_encoded['Mortgage_Thousands'].quantile(0.25)\n",
    "Q3 = df_encoded['Mortgage_Thousands'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define bounds for outliers\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Filter the dataframe to remove outliers\n",
    "df_no_outliers = df_encoded[(df_encoded['Mortgage_Thousands'] >= lower_bound) & (df_encoded['Mortgage_Thousands'] <= upper_bound)]\n",
    "\n",
    "# reset index\n",
    "df_no_outliers.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df_no_outliers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cECZJ8yOq9tM"
   },
   "outputs": [],
   "source": [
    "# Features and target\n",
    "X = df_no_outliers.drop(['Income_Thousands'], axis = 1)  # Use features to predict income\n",
    "y = df_no_outliers['Income_Thousands']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features (important for Neural Network)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sjP7DJoNsNB_",
    "outputId": "39cc348b-9144-4453-f7ee-fd70852f4a5e"
   },
   "outputs": [],
   "source": [
    "# Initialize and train the Linear Regression model\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_lr = lr_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "mse_lr = mean_squared_error(y_test, y_pred_lr)\n",
    "print(f\"Linear Regression Mean Squared Error: {mse_lr}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fM5f8V2OsOnZ",
    "outputId": "a8a8c20f-6cd6-40ae-c991-e23a82d3dcb9"
   },
   "outputs": [],
   "source": [
    "# Initialize the Neural Network model\n",
    "nn_model = Sequential()\n",
    "nn_model.add(Dense(64, input_dim=X_train_scaled.shape[1], activation='relu'))\n",
    "nn_model.add(Dense(32, activation='relu'))\n",
    "nn_model.add(Dense(1))  # Single output for regression\n",
    "\n",
    "# Compile the model\n",
    "nn_model.compile(optimizer='rmsprop', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "nn_model.fit(X_train_scaled, y_train, epochs=10, batch_size=16, validation_data=(X_test_scaled, y_test), verbose=1)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_nn = nn_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "mse_nn = mean_squared_error(y_test, y_pred_nn)\n",
    "print(f\"Neural Network Mean Squared Error: {mse_nn}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jtx9lRVms7SI",
    "outputId": "338bea39-bf71-44b0-fdfa-736ba5e803e6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Define the function to create the neural network model\n",
    "def create_nn_model(optimizer='adam', neurons=64):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, input_dim=X_train_scaled.shape[1], activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(1))  # Output layer for regression\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Create a custom scikit-learn wrapper class for Keras\n",
    "class KerasRegressorCustom(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, build_fn=None, optimizer='adam', neurons=64, epochs=100, batch_size=32, verbose=0):\n",
    "        self.build_fn = build_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.neurons = neurons\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.model = self.build_fn(optimizer=self.optimizer, neurons=self.neurons)\n",
    "        self.model.fit(X, y, epochs=self.epochs, batch_size=self.batch_size, verbose=self.verbose)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "# Wrap the model with KerasRegressorCustom for use in GridSearchCV\n",
    "nn_model = KerasRegressorCustom(build_fn=create_nn_model, epochs=100, batch_size=32, verbose=0)\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid_nn = {\n",
    "    'optimizer': ['adam', 'rmsprop'],  # Optimizer types\n",
    "    'batch_size': [64, 128],  # Batch size\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV with cross-validation\n",
    "grid_search_nn = GridSearchCV(estimator=nn_model, param_grid=param_grid_nn,\n",
    "                              cv=3, scoring='neg_mean_squared_error', verbose=1)\n",
    "grid_search_nn.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Best parameters and model performance\n",
    "print(f\"Best hyperparameters for Neural Network: {grid_search_nn.best_params_}\")\n",
    "best_nn_model = grid_search_nn.best_estimator_\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_nn = best_nn_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "mse_nn = mean_squared_error(y_test, y_pred_nn)\n",
    "print(f\"Neural Network Mean Squared Error: {mse_nn}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JKEhMqsPtIQm",
    "outputId": "06a644f3-6039-4b4a-d024-6dbf73373061"
   },
   "outputs": [],
   "source": [
    "new_customer = {\n",
    "    'Age': 45,\n",
    "    'Experience_Years': 20,\n",
    "    'Sort Code': 1010,\n",
    "    'Family': 3,\n",
    "    'Credit Score': 700,\n",
    "    'Mortgage_Thousands': 150,\n",
    "    'Education_Degree': 1,\n",
    "    'Education_Diploma': 0,\n",
    "    'Education_Masters': 0,\n",
    "    'Personal Loan_No': 0,\n",
    "    'Personal Loan_Yes': 1,\n",
    "    'Securities Account_No': 1,\n",
    "    'Securities Account_Yes': 0,\n",
    "    'CD Account_No': 1,\n",
    "    'CD Account_Yes': 0,\n",
    "    'Online Banking_No': 0,\n",
    "    'Online Banking_Yes': 1,\n",
    "    'CreditCard_No': 0,\n",
    "    'CreditCard_Yes': 1\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "new_customer_df = pd.DataFrame([new_customer])\n",
    "\n",
    "# Apply the same scaler used on the training data\n",
    "new_customer_scaled = scaler.transform(new_customer_df)\n",
    "\n",
    "predicted_income = best_nn_model.predict(new_customer_scaled)\n",
    "print(f\"Predicted Income (in Thousands): {predicted_income[0][0]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X2y_vQvPor5o"
   },
   "source": [
    "## Semantic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qApANPWPnt5Y",
    "outputId": "d2738139-8c4a-4a6e-87cb-2c197ab1ae56"
   },
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "from youtube_comment_downloader import *\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize the downloader\n",
    "downloader = YoutubeCommentDownloader()\n",
    "\n",
    "# Fetch top 10 popular comments\n",
    "comments = downloader.get_comments_from_url(\n",
    "    'https://www.youtube.com/watch?v=PuSQPHOJXNY'\n",
    ")\n",
    "\n",
    "# Convert to list of dictionaries (each comment is a dict)\n",
    "comment_list = list(islice(comments, 100))\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(comment_list)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5-bBNTqxoJdr",
    "outputId": "df495ce3-44f9-4812-9e8e-8f45a23341da"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "import nltk\n",
    "from transformers import pipeline\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 978,
     "referenced_widgets": [
      "bcc8ba4501a94464a7c393c1666509cb",
      "36e014d54b574a0a9f7af8f50c95549b",
      "3b5187ad79c0432a80a862960fd1d1c9",
      "2e9e55a307a94f6aa4ad464db9889ec2",
      "9d8ce6df7f6d47448a1b30012b8a3d5c",
      "03f68783443d4c1b91a90f9a5debf53a",
      "44b3f53db1c246a184d439cabe72a7f7",
      "9aed13d9441d456ea767871292123202",
      "6f8b596f1d8c419b9d9079f847c04f7e",
      "f3e29b77d22149019622a72a0fd89357",
      "39b5cb736e394d7bb637340c4e71426a",
      "c60789205d8d4146ac20c778ee1f2916",
      "e8ba7a881df9410aaa23fcbb6d32c744",
      "8bffc2eb351d44518155e0141cdbc284",
      "69cdb54cf70741f9ae6106f0d2005f75",
      "c5a5da45bcca48fb9c967432104d9648",
      "ab2f7c36bd44453ca6e6706dba91a4b1",
      "d12c24032a6047bf9ad8317608afe709",
      "deea1d6c1ac141c68222fe86baed7aba",
      "94d6784cbe7842c8a2bd2afcdf73f50e",
      "4e55608341f34875a97107d63944ba41",
      "3f0ea93304d94d64a125e47726ed57c7",
      "409cf66fa90c4653b6b2e4936d2fc19f",
      "85402829fd704b108338a3c76e96f161",
      "5e35546f9077478e8cc8f70a32f76eb0",
      "a81044bbf21e4d89b33aceb84e177c49",
      "7e294b6b8a034eb3ad2a9586ace1f045",
      "39f903d27eef43658da00b9faa2463cf",
      "e3a2c5ef330342e19cad8efd59b9d0ed",
      "b81c02361adb4d7c9dce59128afc6ef5",
      "3c84246d4fa34975b9c62ba6962434ca",
      "9b8d19b929c04e6489892368a286eecb",
      "2cccbc6db77a42669f1bac3c2cd82c7e",
      "714ae3ece67b4d08ad55e660f5f38004",
      "5d750c01bc6a42cebc01185c6b7792d6",
      "285b7849d930496d9a35bef0ed6acd15",
      "6b586bbdfcc34d19ae0e9f4a38d007b7",
      "0d4735ee4c8c4a12aaa74ba52986a2c2",
      "5b1b4dbba4ab4c48b52dddefd6b10fa4",
      "8f67f62462224e1aad05df7d9c69bea7",
      "ae5338c48af84528a70eec0e0f730094",
      "b3af73be15474a248f7374e119b4fbd3",
      "6ba8b7a3226e408d95ad1b6809dedbc4",
      "4ae847c091804fd99618359e28121312",
      "cbf81f18a2884b8b83930fb16bb9d8ee",
      "70b4ce74588f454da312e5c67548e60f",
      "07ff0b3c3435416d86947e0b355f15a1",
      "73b3b04bd43a41da81c84be97ff9e249",
      "6e089ae7a77541bdb4d6daf189e1a737",
      "3952d9c1b47e4b8c82f7e016f8d658ee",
      "b7973a52863e4bf8a511b6b0cf276403",
      "f5cf9c3959884e1bafe80742c26c3509",
      "27275c9459b04e02887bd82fb6b9b04d",
      "dc8498852d8f4a248781c64b90861643",
      "60f378ac2d78437494bd6a3aba7f83fe",
      "66ad3592067b4d3fab10b313e618a020",
      "ac779d127d7848409735b13f0ce9d26d",
      "2677ece928c54acdbe5fb338b2b0d7f4",
      "6e0c2057c2574ecd9762cb62db54ba86",
      "c95da21c52594894b096b17059a6f699",
      "40b294987683477a8384e82d1745d7f2",
      "4d1d8aa949de482498b03b250e110c86",
      "1fea0c5280884276ab5cfefe0635087b",
      "5b25dd5a03f54663bb6171e41d7c56d2",
      "8181160c37d2477d8ca89b0dfe0541bd",
      "bcb2fec47548436499373ba2c10bf89c",
      "a6b7ae9974c54f6fbb7a9230c58e8d4b",
      "7fad773317954359b90fc5a33f6fc385",
      "f33aeb9aab98437eaa274f823fbeec80",
      "6d54c9da1b9047efa31d9254eb6894a3",
      "4a58f9b5fd8a4c1c93d9e6d96e0db7d3",
      "ed6da1e5df554f88bb5271d9c46b3aff",
      "6eab31428b1644a6a379a7c208c0299c",
      "4b98081244a74dac938f9b42f2581a18",
      "bb3ab115a4e446a8b19bd646ba97c5e6",
      "62926e06e8914a63b0024123ee19badd",
      "920bad96501747fb9b8aac1670ce7d80",
      "0bc4fa7d3093424b8253ce06a6d581ee",
      "eaef64cc384f4f07bbaabe3a8bb61f2d",
      "78ad3bd7251141aba5dc976614b1ab9a",
      "705af47c4311410a97f00c7926e2f226",
      "d79e1c3d0a5142678c8fc990211965aa",
      "aff5158e3a28446aa883d09b324aa52a",
      "38a94c37648e46bdb979e8ee74bc5fc8",
      "2663702d5767488a8269d2398a317aa9",
      "46e5c9fda3494acd80ada10e1dd7896f",
      "9d694fd1df8646e8be0ed5e929762aee",
      "9d1ecb0f0dac4189b716b8de43df1a91"
     ]
    },
    "id": "EAgOvvw8oNuP",
    "outputId": "3f807110-59a6-47ef-a25a-dcecdfc9a1f1"
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)  # remove URLs\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)  # remove punctuation & digits\n",
    "    text = text.lower()  # lowercase\n",
    "    return text.strip()\n",
    "\n",
    "df['clean_text'] = df['text'].astype(str).apply(clean_text)\n",
    "\n",
    "def get_sentiment(text):\n",
    "    polarity = TextBlob(text).sentiment.polarity\n",
    "    if polarity > 0.1:\n",
    "        return 'Positive'\n",
    "    elif polarity < -0.1:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "df['Sentiment'] = df['clean_text'].apply(get_sentiment)\n",
    "\n",
    "emotion_model = pipeline(\"text-classification\", model=\"j-hartmann/emotion-english-distilroberta-base\", top_k=1)\n",
    "\n",
    "def classify_emotion(text):\n",
    "    try:\n",
    "        result = emotion_model(text)[0]\n",
    "        return result['label']\n",
    "    except:\n",
    "        return 'unknown'\n",
    "\n",
    "# Apply only to non-empty cleaned texts\n",
    "df['Emotion'] = df['clean_text'].apply(lambda x: classify_emotion(x) if len(x) > 3 else 'unknown')\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(data=df, x='Sentiment', palette='Set2', order=['Positive', 'Neutral', 'Negative'])\n",
    "plt.title(\"Sentiment Distribution (3 Categories)\")\n",
    "plt.xlabel(\"Sentiment\")\n",
    "plt.ylabel(\"Number of Comments\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "NP30X5OmpSIW",
    "outputId": "a5b9fcdf-f5cf-437e-fe35-b6ae5e80f352"
   },
   "outputs": [],
   "source": [
    "text_all = \" \".join(df['clean_text'].tolist())\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text_all)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title(\"Word Cloud of YouTube Comments\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
